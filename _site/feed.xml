<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-12T03:04:27-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">sonia joseph</title><subtitle>machine learning, computational neuroscience, &amp; natural language processing essays by sonia joseph</subtitle><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><entry><title type="html">Papers for Vision Transformers (ViT) and Mechanistic Interpretability</title><link href="http://localhost:4000/machine%20learning/vision%20transformers/vit-papers/" rel="alternate" type="text/html" title="Papers for Vision Transformers (ViT) and Mechanistic Interpretability" /><published>2024-03-11T00:00:00-07:00</published><updated>2024-03-11T00:00:00-07:00</updated><id>http://localhost:4000/machine%20learning/vision%20transformers/vit-papers</id><content type="html" xml:base="http://localhost:4000/machine%20learning/vision%20transformers/vit-papers/"><![CDATA[<p>Winter 2023<br />
Advised by Dr. Blake Richards</p>

<p>Here is an incomplete list of papers I found helpful to read in developing more context for running mechanistic interpretability on vision transformers (ViTs).</p>

<h2 id="vision-transformers">Vision Transformers</h2>

<ol>
  <li>Ibrahim Alabdulmohsin et al. <a href="https://arxiv.org/pdf/2305.13035.pdf">‚ÄúGetting ViT in Shape: Scaling Laws for Compute-Optimal Model Design‚Äù</a>. In: <em>arXiv preprint arXiv:2305.13035</em> (2023).</li>
  <li>Timoth√©e Darcet et al. <a href="https://arxiv.org/pdf/2309.16588.pdf">‚ÄúVision transformers need registers‚Äù</a>. In: <em>arXiv preprint arXiv:2309.16588</em> (2023).</li>
  <li>Alexey Dosovitskiy et al. <a href="https://openreview.net/pdf?id=YicbFdNTTy">‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale‚Äù</a>. In: <em>arXiv preprint arXiv:2010.11929</em> (2020).</li>
  <li>Salman Khan et al. <a href="https://arxiv.org/pdf/2101.01169.pdf">‚ÄúTransformers in vision: A survey‚Äù</a>. In: <em>ACM computing surveys (CSUR)</em> 54.10s (2022), pp. 1‚Äì41.</li>
  <li>Muhammad Muzammal Naseer et al. <a href="https://openreview.net/pdf?id=o2mbl-Hmfgd">‚ÄúIntriguing properties of vision transformers‚Äù</a>. In: <em>Advances in Neural Information Processing Systems</em> 34 (2021), pp. 23296‚Äì23308.</li>
  <li>Maithra Raghu et al. <a href="https://arxiv.org/pdf/2108.08810.pdf">‚ÄúDo vision transformers see like convolutional neural networks?‚Äù</a>. In: <em>Advances in Neural Information Processing Systems</em> 34 (2021), pp. 12116‚Äì12128.</li>
  <li>Daquan Zhou et al. <a href="https://proceedings.mlr.press/v162/zhou22m/zhou22m.pdf">‚ÄúUnderstanding the robustness in vision transformers‚Äù</a>. In: <em>International Conference on Machine Learning. PMLR.</em> 2022, pp. 27378‚Äì27394.</li>
</ol>

<h2 id="feature-visualization-and-interpretability">Feature Visualization and Interpretability</h2>

<ol>
  <li>Shan Carter et al. <a href="https://distill.pub/2019/activation-atlas/">‚ÄúExploring neural networks with activation atlases‚Äù</a>. In: <em>Distill.</em> (2019).</li>
  <li>Haozhe Chen et al. <a href="https://arxiv.org/pdf/2310.10591.pdf">‚ÄúInterpreting and Controlling Vision Foundation Models via Text Explanations‚Äù</a>. In: <em>arXiv preprint arXiv:2310.10591</em> (2023).</li>
  <li>Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. <a href="https://arxiv.org/pdf/2310.05916.pdf">‚ÄúInterpreting CLIP‚Äôs Image Representation via Text-Based Decomposition‚Äù</a>. In: <em>arXiv preprint arXiv:2310.05916</em> (2023).</li>
  <li>Robert Geirhos et al. <a href="https://arxiv.org/pdf/2306.04719.pdf">‚ÄúDon‚Äôt trust your eyes: on the (un) reliability of feature visualizations‚Äù</a>. In: <em>arXiv preprint arXiv:2306.04719</em> (2023).</li>
</ol>

<h2 id="mechanistic-interpretability">Mechanistic Interpretability</h2>

<ol>
  <li>Kumar K Agrawal et al. <a href="https://arxiv.org/pdf/2304.14997.pdf">‚ÄúŒ±-ReQ: Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay‚Äù</a>. In: <em>Advances in Neural Information Processing Systems</em> 35 (2022), pp. 17626‚Äì17638.</li>
  <li>Trenton Bricken et al. <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">‚ÄúTowards monosemanticity: Decomposing language models with dictionary learning‚Äù</a>. Transformer Circuits Thread, 2023.</li>
  <li>Arthur Conmy et al. <a href="https://arxiv.org/pdf/2209.10652.pdf">‚ÄúTowards automated circuit discovery for mechanistic interpretability‚Äù</a>. In: <em>arXiv preprint arXiv:2304.14997</em> (2023).</li>
  <li>Nelson Elhage et al. <a href="https://transformer-circuits.pub/2021/framework/index.html">‚ÄúA mathematical framework for transformer circuits‚Äù</a>. In: Transformer Circuits Thread 1 (2021).</li>
  <li>Nelson Elhage et al. <a href="https://arxiv.org/ftp/arxiv/papers/2209/2209.10652.pdf">‚ÄúToy models of superposition‚Äù</a>. In: <em>arXiv preprint arXiv :2209.10652</em> (2022).</li>
  <li>Kevin Wang et al. <a href="https://openreview.net/pdf?id=NpsVSN6o4ul">‚ÄúInterpretability in the wild: a circuit for indirect object identification in gpt-2 small‚Äù</a>. In: <em>arXiv preprint arXiv:2211.00593</em> (2022).</li>
</ol>

<h2 id="training-dynamics-and-phase-transitions">Training Dynamics and Phase Transitions</h2>

<ol>
  <li>Eric J Michaud et al. <a href="https://arxiv.org/pdf/2303.13506.pdf">‚ÄúThe quantization model of neural scaling‚Äù</a>. In: <em>arXiv preprint arXiv:2303.13506</em> (2023).</li>
  <li>Neel Nanda et al. <a href="https://arxiv.org/pdf/2301.05217.pdf">‚ÄúProgress measures for grokking via mechanistic interpretability‚Äù</a>. In: <em>arXiv preprint arXiv:2301.05217</em> (2023).</li>
  <li>Catherine Olsson et al. <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">‚ÄúIn-context learning and induction heads‚Äù</a>. In: <em>arXiv preprint arXiv:2209.11895</em> (2022).</li>
</ol>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="machine learning" /><category term="vision transformers" /><category term="machine learning" /><category term="vision transformers" /><summary type="html"><![CDATA[Papers that give context when exploring mechanistic interpretability on vision transformers.]]></summary></entry><entry><title type="html">Update 1</title><link href="http://localhost:4000/unfinished-thoughts/unfinished-thoughts/" rel="alternate" type="text/html" title="Update 1" /><published>2021-10-12T00:00:00-07:00</published><updated>2021-10-12T00:00:00-07:00</updated><id>http://localhost:4000/unfinished-thoughts/unfinished-thoughts</id><content type="html" xml:base="http://localhost:4000/unfinished-thoughts/unfinished-thoughts/"><![CDATA[<h2 id="im-experimenting-with-publicly-blogging-about-my-decision-making">I‚Äôm experimenting with publicly blogging about my decision-making</h2>

<p>I‚Äôve decided to be more public about my decision processes and brainstorming at this stage in life, opposed to operating in stealth.</p>

<p>The upside is that I‚Äôll get more clear feedback on my direction, which enables faster course-correction.</p>

<p>The downside is that I do something unpopular and contrarian, and the feedback could erode at my morale. Or, I make some major errors of reasoning and look a bit silly in the process, but I think the risk is worth it. At very least, I‚Äôll be more accurately resuscitated when this webpage is scraped for the industrial-scale language models.</p>

<p><em>Note: these thoughts are stream-of-consciousness, with minimal editing, and may contain grammatical and spelling errors. Find my polished thoughts elsewhere.</em></p>

<h2 id="moving-to-montreal-after-the-bay">Moving to Montreal after the Bay</h2>

<p>I recently moved to Montreal to research reinforcement learning at Mila, the largest academic institute of machine learning. After several years immersed in startup culture in the Bay, the cultural switch was interesting.</p>

<h3 id="some-observations">Some observations:</h3>
<ul>
  <li><strong>Researchers not familiar with startups evaluate their possible startup competence on researcher hierarchies.</strong> Many talented researchers here are interested in startups but confused as to where to begin. Some researchers also seem to think that you have to be way more technically skilled than you actually have to be to start a company‚Äì e.g. you have to finish your CS PhD before starting a company. Someone thought Sam Altman had an AI PhD from Stanford (he doesn‚Äôt, he dropped out of Stanford as an undergrad). I think this impression comes from confusing value hierarchies. In science, competence is signalled through publications and completing a PhD is a big deal. However, in startups, if you‚Äôre starting a company this stuff matters way less. What matters more is your ability to recruit talented people to join you, raise capital, and coordinate the show.</li>
  <li><strong>Why Bay Area culture makes the place conducive for startups.</strong> There are several cultural assumptions in the Bay Area that make starting a company easier: in many communities, you‚Äôre simply expected to do it eventually. Networks are incredibly dense: within your graph clique, you feel like everyone really does know everyone. Your friends are a jumble of founders, VCs, engineers, and researchers. Risk tolerance and friendliness are both high, which leads to both companies and cults.</li>
  <li><strong>Government vs billionaire funding.</strong> Billionaire funding is seen as very bizarre by many Canadians, who actually seem to trust their government. Some researchers want more profitable companies to arise from their labs so that the government pours more funding into AI. This thinking differs from that in the Bay Area, where some researchers start companies with the hope of independently funding their research through private investors or through revenue.</li>
  <li>I‚Äôve found the startup community here to be very welcoming and open-minded. I have not raised funding in Montreal though I hear that valuations are far lower than those in the US.</li>
  <li>The government subsidizes a huge slew of programs to make hiring interns pretty inexpensive for early-stage founders.</li>
</ul>

<h2 id="what-to-found-next-stay-in-ai-enter-web-3-frontier-or-something-else">What to found next: stay in AI, enter web 3 frontier, or something else?</h2>

<p>I‚Äôm exploring the startup space as to what to fund next. I‚Äôm torn between staying in my field (AI) and making a bid in web 3.</p>

<p>Philosophically, I am aligned with creating AI companies to explore the nature of intelligence as a fundamentally important metaphysical question that is dear to my heart. My research and engineering background sit at the intersection of computational neuroscience and artificial intelligence: I‚Äôve worked for neuro labs and search companies.</p>

<p>However, AI is maturing as a sector. My personality loves emerging sectors. I love the questioning of all fundamental assumptions and rederiving structures that exist in the world. In conversations with friends about DAOs, we end up rederiving existing corporate structures: <em>so that‚Äôs why things are the way they are.</em></p>

<p>In web 3, I also smell more potential upside. However, investing huge amounts of time into web 3 is more aligned with philosophically questioning the nature of government, incentives, mechanism design. All good stuff, except my aesthetic preferences lean toward the nature of intelligence.</p>

<h3 id="to-financially-optimize">To financially optimize?</h3>

<p>The other variable is whether to financially optimize‚Äì not for personal wealth, but enough to significantly become a significant player and invest in bets that no one else would otherwise invest in. Given that 84% of EA is funded by mega-billionaires, I suspect that financially optimizing would increase the preference-set of things that are funded.</p>

<p><em>On EA</em>
Effective Altruism calls this model ‚Äúearn to give.‚Äù In 2016 when I was more enmeshed with the community, I got the impression ‚Äúearn to give‚Äù meant take a stable job and invest 20% of your income. However, friends who are more embedded in the movement tell me that times are changing and EA members recognize the startup world‚Äôs power law.</p>

<p>Thus it‚Äôs not clear to me why EA hasn‚Äôt founded something like Y-Combinator which gets most of its return from the power law unicorn of AirBnB. An EA-adjacent friend hypothesizes that entrepreneurs ‚Äúlike secret knowledge‚Äù and EA doesn‚Äôt feel very ‚Äúsecret‚Äù anymore‚Äì so this may be an aesthetic preference.</p>

<p><em>Which companies optimize most financially?</em>
If financially optimizing is the play, the next question is which companies would be the most profitable if created. I‚Äôm steadily looking at the DeFi sector, but we‚Äôre in a web 3 bull run and the market is quickly saturating. It‚Äôs hard to fully assess because the space as ballooned rapidly very quickly. To gather more data about web 3 as a bet, I‚Äôve joined communities like KERNEL and will soon be spending some time in NYC.</p>

<p>Staying in AI, bets that are both financially and philosophically aligned look like building an industrial-scale language model, screen capture, or some other company with giant amounts of data and compute. It‚Äôs not obvious to me why GPT-3 has basically zero US competitors. However, the outcome here looks pretty cleanly like ‚Äúget acquihired by Google or Microsoft‚Äù and it‚Äôs not clear that this is a great outcome. AI is a maturing and gated world where web 3 is not.</p>

<p><strong>The tl;dr is that AI is philosophically aligned and profitable, but web 3 could be far more profitable, and while less aligned with my philosophy, more aligned with my personality.</strong></p>

<h2 id="break-from-ea">Break from EA</h2>

<p>I‚Äôve been writing a lot in stealth about transhumanism, and new types of movements cephalized around this body of ideas. Over the past decade, EA has dominated as a main aggregator of transhumanist thought. There are probably thousands of smaller transhumanist communities embedded about the world that I‚Äôm less familiar with, including startup/VC communities that invest in deeptech but don‚Äôt explicitly identify as transhumanist.</p>

<p>I‚Äôm interested in creating a movement that‚Äôs decorrelated from Effective Altruism to variance max value systems in the space. I notice fear in doing so (‚Äúwhat will the old guard transhumanists think?‚Äù) but increasingly, I am realizing the possible upside, if done well, is worth it.</p>

<p>In writing the whitepaper for the movement, I face the balance between artistic (Nietzsche) and academic. There are strong trade-offs between the two. Religious-esque blog writing is no longer as fashionable as dry-sounding whitepapers‚Äì perhaps we live in a hyperrationalistic age.</p>

<h2 id="comments">Comments</h2>
<p>See responses on Twitter <a href="https://twitter.com/soniajoseph_/status/1448139915821080581">here</a></p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="unfinished-thoughts" /><category term="unfinished-thoughts" /><summary type="html"><![CDATA[Thoughts on Montreal after the Bay Area]]></summary></entry><entry><title type="html">Optic Flow and PWC-Net</title><link href="http://localhost:4000/deep%20learning/neuroscience/optic-flow-pwc-net/" rel="alternate" type="text/html" title="Optic Flow and PWC-Net" /><published>2021-01-10T00:00:00-08:00</published><updated>2021-01-10T00:00:00-08:00</updated><id>http://localhost:4000/deep%20learning/neuroscience/optic-flow-pwc-net</id><content type="html" xml:base="http://localhost:4000/deep%20learning/neuroscience/optic-flow-pwc-net/"><![CDATA[<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vS3_GN-Ep7AL8irD7bAKjqAA-SNcm9XNIapxO3Hbwkn6-UiKb5TJqV6YErmfxSSczifS3AyvE9BFdZY/embed?start=false&amp;loop=false&amp;delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="deep learning" /><category term="neuroscience" /><category term="neuroscience" /><category term="machine learning" /><category term="deep learning" /><category term="artificial intelligence" /><category term="resources" /><category term="presentation" /><category term="computational neuroscience" /><summary type="html"><![CDATA[Presentation on optic flow, PWC-Net, and a possible application to birds and mice for Janelia]]></summary></entry><entry><title type="html">Meta-learning Research Overview and Paper Group</title><link href="http://localhost:4000/machine%20learning/meta-learning-research-overview/" rel="alternate" type="text/html" title="Meta-learning Research Overview and Paper Group" /><published>2020-12-01T00:00:00-08:00</published><updated>2020-12-01T00:00:00-08:00</updated><id>http://localhost:4000/machine%20learning/meta-learning-research-overview</id><content type="html" xml:base="http://localhost:4000/machine%20learning/meta-learning-research-overview/"><![CDATA[<p><em>Incomplete and in progress. Feel free to comment paper recommendations below.</em></p>

<h1 id="radical-meta-learning-paper-group">Radical Meta-Learning Paper Group</h1>

<h2 id="faq">FAQ</h2>

<p><strong><em>What is the purpose of this reading group?</em></strong></p>

<p>There is a trend for algorithms to move away from handcrafted parameters and toward minimizing the human-specified inductive bias as much as possible. Meta-learning is a continuation of this trend.</p>

<p>In this reading group, we want to gain a deep understanding of ‚Äúradical‚Äù meta-learning, i.e., algorithms that learn how to learn how to learn ‚Äì and on till the nth meta-level.</p>

<p><strong><em>Why ‚Äúradical‚Äù?</em></strong></p>

<p>‚ÄúRadical‚Äù is a word taken from Jurgen Schmidhuber to distinguish this branch of meta-learning from current transfer learning techniques (e.g. MAML). For example, this difference could be between learning gradient descent itself (radical meta-learning) and optimizing gradient descent across many data distributions (a type of meta-learning / transfer learning). We want to study the first camp: learning as much of the algorithm as possible from scratch.</p>

<p><strong><em>How do I join?</em></strong></p>

<p>Shoot a quick email to Sonia Joseph at smjoseph@alumni.princeton.edu</p>

<p><strong><em>What is the format?</em></strong></p>

<p>One person will sign up to present a paper, which everyone will read. We‚Äôll collectively take notes on a Google doc before and during the presentation. The presenter will go through the paper, and we‚Äôll liberally discuss along the way. Some of the papers are long, so please feel free to come anyway even if you haven‚Äôt finished it.</p>

<h3 id="phase-1">Phase 1:</h3>

<h3 id="sample-reading-list-please-put-your-name-next-to-what-you-would-like-to-present">Sample reading list [please put your name next to what you would like to present]</h3>

<ol>
  <li><em>AIXI. <a href="https://arxiv.org/abs/cs/0004001">Link.</a></em></li>
  <li><em>Evolutionary principles in self-referential learning. (On learning how to learn: The meta-meta-‚Ä¶ hook.). 1987.</em> <a href="http://people.idsia.ch/~juergen/diploma1987ocr.pdf">Link</a>. <strong>Presenter: Sonia Joseph</strong></li>
  <li><em>Ultimate Cognition a la Godel. 2009.</em> <a href="http://people.idsia.ch/~juergen/ultimatecognition.pdf">Link</a>. <strong>Presenter: Sid</strong></li>
  <li><em>Reinforcement Learning with Self-Modifying Policies. 1997. <a href="http://people.idsia.ch/~juergen/ssabook/ssabook.html">Link</a>.</em></li>
  <li><em>Optimally Ordered Problem Solver. 2004. <a href="http://people.idsia.ch/~juergen/oopsweb/oopsweb.html">Link</a>.</em></li>
  <li><em>AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence. <a href="https://arxiv.org/pdf/1905.10985.pdf">Link</a>. <strong>Presenter</strong>: Acyr<strong>.</strong></em></li>
  <li><em>Improving Generalization in Meta Reinforcement Learning using Learned Objectives. 2019.</em> <a href="https://arxiv.org/abs/1910.04098">Link</a>. <strong>Presenter</strong>: Rob/Louis Kirsch (?)</li>
  <li><em>Discovering Reinforcement Learning Algorithms. 2020. <a href="https://arxiv.org/abs/2007.08794">Link.</a></em> <strong>Presenter</strong>: Rob</li>
  <li><em>Meta-Gradient Reinforcement Learning with an Objective Discovered Online. 2020. <a href="https://arxiv.org/pdf/2007.08433.pdf">Link</a>.</em> <strong>Presenter</strong>: Rob</li>
</ol>

<h3 id="schedule-tentative">Schedule [tentative]</h3>

<p><em>We will have four meetings.This is rough‚Äî we‚Äôll finalize the schedule with a whenisgood.</em></p>

<ol>
  <li><strong>Seminar 1. Sunday Jan 29, 6:00 pm GMT</strong>
    <ol>
      <li>Whenisgood for first meeting: <a href="https://whenisgood.net/78wbi7k">https://whenisgood.net/78wbi7k</a></li>
      <li><strong>Paper</strong>: <em>Evolutionary principles in self-referential learning. (On learning how to learn: The meta-meta-‚Ä¶ hook.). 1987.</em> <a href="http://people.idsia.ch/~juergen/diploma1987ocr.pdf">Link</a>.</li>
      <li><strong>Presenter</strong>: Sonia Joseph
        <ol>
          <li><strong>Slides:</strong> <a href="https://docs.google.com/presentation/d/14uhiWYJUOILFybIEKvaiZPs5y8xuF6YgzINygE5E9Ys/edit#slide=id.p">https://docs.google.com/presentation/d/14uhiWYJUOILFybIEKvaiZPs5y8xuF6YgzINygE5E9Ys/edit#slide=id.p</a></li>
        </ol>
      </li>
      <li><strong>Notes</strong>: <a href="https://docs.google.com/document/d/1HBzO4YEm2L8drf82UPxCRDlAqiRzMJcJW77ccyCoLW4/edit?usp=sharing">https://docs.google.com/document/d/1HBzO4YEm2L8drf82UPxCRDlAqiRzMJcJW77ccyCoLW4/edit?usp=sharing</a></li>
    </ol>
  </li>
  <li><strong>Seminar 2. Feb 14th, 6:00 pm GMT</strong>
    <ol>
      <li><strong>Paper</strong>: Ultimate Cognition a la Godel.</li>
      <li><strong>Presenter</strong>: Sid</li>
      <li><strong>Notes</strong>: <a href="https://docs.google.com/document/d/1Wg4Aoy9qhhaBpPrUxp9KNhBf2f3-HwiagCN4arQhJ5E/edit#">https://docs.google.com/document/d/1Wg4Aoy9qhhaBpPrUxp9KNhBf2f3-HwiagCN4arQhJ5E/edit#</a></li>
    </ol>
  </li>
  <li><strong>Seminar 3. Feb 26 weekend.</strong>
    <ol>
      <li><strong>Paper</strong>: AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence. <a href="https://arxiv.org/pdf/1905.10985.pdf">Link</a>.</li>
      <li><strong>Presenter</strong>: Acyr.</li>
      <li><strong>Notes</strong>: <em>Coming soon.</em></li>
    </ol>
  </li>
  <li><strong>Seminar 4. March 12th weekend.</strong>
    <ol>
      <li><strong>Paper:</strong> Meta-Policy Gradients - Papers: 6,7,8.</li>
      <li><strong>Presenter</strong>: Rob.</li>
      <li><strong>Notes</strong>: Checkout this fantastic <a href="https://roberttlange.github.io/posts/2020/12/meta-policy-gradients/">blog post</a> üë®‚Äçüîß</li>
    </ol>
  </li>
</ol>

<h2 id="phase-2">Phase 2</h2>

<h3 id="reading-list--ai-ga-related">Reading list  [AI-GA related]</h3>

<ol>
  <li><em>Generative teaching networks: learning to teach by generating synthetic training data.</em> <a href="https://arxiv.org/abs/1912.07768">Link</a></li>
  <li><em>Differentiable plasticity: training plastic neural networks with backpropagation.</em> <a href="https://arxiv.org/abs/1804.02464">Link</a></li>
  <li><em>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions.</em> <a href="https://arxiv.org/abs/1901.01753">Link</a></li>
</ol>

<h3 id="schedule-tentative-1">Schedule [tentative]</h3>

<ol>
  <li><strong>Seminar 1. April 11th, 6:00 pm GMT</strong>
    <ol>
      <li>POET by Anushan.</li>
    </ol>
  </li>
  <li><strong>Seminar 2. April 25th, 6:00 pm GMT</strong>
    <ol>
      <li>Differentiable Plasticity by Harsha</li>
    </ol>
  </li>
</ol>

<h2 id="research-papers-by-concept">Research Papers by Concept</h2>

<h3 id="proof-search">Proof-Search</h3>
<ul>
  <li><a href="http://people.idsia.ch/~juergen/ultimatecognition.pdf">Ultimate Cognition a la Godel</a>(2009)</li>
  <li><a href="http://people.idsia.ch/~juergen/oopsweb/oopsweb.html">Optically Ordered Problem-Solver</a> (2004)</li>
</ul>

<h3 id="transfer-learning">Transfer Learning</h3>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/2007.08433.pdf">Meta-Gradient Reinforcement Learning with an Online Objective Discovered Online</a> (2020)</li>
  <li><a href="https://arxiv.org/pdf/2007.08794.pdf">Discovering Reinforcement Learning Algorithms</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/1910.04098">Improving Generalization in Meta Reinforcement Learning using Learned Objectives</a> (2019)</li>
</ul>

<h3 id="generative-models">Generative Models</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1905.10985.pdf">AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence</a> (2020)</li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="machine learning" /><category term="meta-learning" /><category term="machine learning" /><category term="artificial intelligence" /><category term="resources" /><summary type="html"><![CDATA[Research overview of meta-learning by researcher and institution.]]></summary></entry><entry><title type="html">Fall 2020 NLP Journal Club: Apply to Join!</title><link href="http://localhost:4000/projects/nlp-journal-club/" rel="alternate" type="text/html" title="Fall 2020 NLP Journal Club: Apply to Join!" /><published>2020-08-15T00:00:00-07:00</published><updated>2020-08-15T00:00:00-07:00</updated><id>http://localhost:4000/projects/nlp-journal-club</id><content type="html" xml:base="http://localhost:4000/projects/nlp-journal-club/"><![CDATA[<p><em>Update: Applications are now closed, but feel free to email me if you‚Äôd like to connect for a future journal club.</em></p>

<p>Our Summer 2020 Reinforcement Learning Journal Club was a success, so we‚Äôre excited to found another one. I‚Äôm starting a biweekly journal club focused on women in AI and modern NLP techniques with <a href="https://www.pujaarajan.com/">Pujaa Rajan</a>. Apply to join <a href="https://docs.google.com/forms/d/1LCqxx_mZ3SbJjxX31oTxMn1GyDhq5556PWEsSBJ2QVo/edit">here</a>!</p>

<h3 id="eligibility">Eligibility:</h3>

<p>Currently work as a machine learning or deep learning engineer or researcher.</p>

<p>Female and/or underrepresented group in STEM.</p>

<p>Attendance required for all 3 meetings.</p>

<h3 id="papers">Papers:</h3>

<p><em>We will voted on 4 of them.</em></p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1906.08237.pdf">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1909.11942v1.pdf">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1908.04577">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
  </li>
</ul>

<h3 id="notes">Notes:</h3>

<p><em>Updated w/ notes after each journal club.</em></p>

<p>Seminar 1:</p>

<p><em>Coming soon.</em></p>

<p>Seminar 2:</p>

<p><em>Coming soon.</em></p>

<p>Seminar 3:</p>

<p><em>Coming soon.</em></p>

<p>Seminar 4:</p>

<p><em>Coming soon.</em></p>

<h3 id="dates">Dates:</h3>

<p>Saturday, September 5, 2020</p>

<p>Saturday, September 12, 2020</p>

<p>Saturday, September 19, 2020</p>

<p>Saturday, September 26, 2020</p>

<h3 id="time">Time:</h3>

<p>8:00 - 9:00 PM PST</p>

<p>If you‚Äôd like to be a part of it, please apply <a href="https://docs.google.com/forms/d/1LCqxx_mZ3SbJjxX31oTxMn1GyDhq5556PWEsSBJ2QVo/edit">here</a>!</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="projects" /><category term="machine learning" /><category term="natural language" /><category term="processing journal club" /><summary type="html"><![CDATA[Founding a Deep NLP journal club for women in AI.]]></summary></entry><entry><title type="html">Summer 2020 Reinforcement Learning Journal Club: Apply to Join!</title><link href="http://localhost:4000/projects/reinforcement-learning-journal-club/" rel="alternate" type="text/html" title="Summer 2020 Reinforcement Learning Journal Club: Apply to Join!" /><published>2020-06-05T00:00:00-07:00</published><updated>2020-06-05T00:00:00-07:00</updated><id>http://localhost:4000/projects/reinforcement-learning-journal-club</id><content type="html" xml:base="http://localhost:4000/projects/reinforcement-learning-journal-club/"><![CDATA[<p><em>Update: Applications are now closed, but feel free to email me if you‚Äôd like to connect for a future journal club.</em></p>

<p>I‚Äôm founding reinforcement learning paper club to meet once a month this summer with <a href="https://www.pujaarajan.com/">Pujaa Rajan</a>. We‚Äôre looking for around 5 people to add to our  group. Apply <a href="https://docs.google.com/forms/d/1JZ4qzjxKOy4Jnrvd-63KDPeA65ZqPWoTUehEg5AuEys/edit">here</a>.</p>

<h3 id="eligibility">Eligibility:</h3>

<p>Currently work as a machine learning or deep learning engineer.</p>

<p>Attendance required for all 3 meetings.</p>

<h3 id="papers">Papers:</h3>

<p><em>Updated w/ notes after each journal club.</em></p>

<p>Seminar 1:</p>

<p>Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. Kumar et al. 2019. <a href="https://arxiv.org/pdf/1906.00949.pdf">Paper</a>.</p>

<p>Seminar 2:</p>

<p>Reinforcement Learning with Augmented Data. Laskin et al. 2020. <a href="https://arxiv.org/pdf/2004.14990.pdf">Paper</a>. <a href="https://docs.google.com/document/d/1QyB4nyQX7x7AS1PcsfkRfRqYfkdhJKey0zvAMoYZNtA/edit?usp=sharing">Notes</a>.</p>

<p>Seminar 3:</p>

<p>Discovering Reinforcement Learning Algorithms. Oh et al. 2020. <a href="https://arxiv.org/pdf/2007.08794.pdf">Paper</a>. <a href="https://docs.google.com/document/d/1Ve1dzdAppQUEb2MxlmhL_HpH0gG5oCdn3at3bnonnuQ/edit?usp=sharing">Notes</a>.</p>

<h3 id="dates">Dates:</h3>

<p>Saturday, June 20, 2020</p>

<p>Saturday, July 18, 2020</p>

<p>Saturday, August 15, 2020</p>

<h3 id="time">Time:</h3>

<p>8:00 - 9:00 PM PST</p>

<p>If you‚Äôd like to be a part of it, please apply <a href="https://docs.google.com/forms/d/1JZ4qzjxKOy4Jnrvd-63KDPeA65ZqPWoTUehEg5AuEys/edit">here</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="projects" /><category term="machine learning" /><category term="reinforcement learning" /><category term="journal club" /><summary type="html"><![CDATA[Founding a reinforcement learning journal club, welcome to join!]]></summary></entry><entry><title type="html">Emergence: A Library of Hopfield Networks</title><link href="http://localhost:4000/machine%20learning/emergence/" rel="alternate" type="text/html" title="Emergence: A Library of Hopfield Networks" /><published>2020-05-30T00:00:00-07:00</published><updated>2020-05-30T00:00:00-07:00</updated><id>http://localhost:4000/machine%20learning/emergence</id><content type="html" xml:base="http://localhost:4000/machine%20learning/emergence/"><![CDATA[<h2 id="welcome-to-emergence">Welcome to Emergence</h2>

<p>This is a research library to study the emergent properties of undirected neural networks, including Hopfield networks and Boltzmann machines.</p>

<p><em>Access the library <a href="https://github.com/soniajoseph/Emergence">here</a></em>.</p>

<p><em>To read more about Hopfield networks, see the primer I wrote <a href="https://soniajoseph.github.io/computational%20neuroscience/neural-dynamics-primer/">here</a>.</em></p>

<h2 id="setup">Setup</h2>

<p>Set up your local environment and download the requirements.txt.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">venv</span> <span class="n">env</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>
</code></pre></div></div>

<p>See /tests for sample use cases.</p>

<h2 id="hopfield-networks">Hopfield Networks</h2>

<p><img src="/assets/images/posts/FMNIST.png" width="500" class="center" /></p>

<p>Hopfield networks are fascinating one-shot data-denoisers. We train the network to ‚Äúremember‚Äù the top row of fashion MNIST images using Hebbian learning. The network does not store the actual image, but encodes information of the image in its weights.</p>

<p>Then, we add noise to the image, randomly setting 30% of the pixels to the opposite value. When we feed each random image into the pre-trained Hopfield network, we get the original image back (bottom row)!</p>

<h3 id="example-on-fmnist">Example on FMNIST</h3>

<table style="width:100%">
  <tr>
    <td><b>Original</b></td>
    <td><img src="/assets/images/posts/FMNIST_orig.png" width="100" /></td>
  </tr>
  <tr>
    <td><b>Noisy</b></td>
    <td>&lt;<img src="/assets/images/posts/FMNIST_test.png" width="100" /></td>
  </tr>
  <tr>
    <td><b>Reconstructed</b></td>
    <td><img src="/assets/images/posts/FMNIST_return.png" width="100" /></td>
  </tr>
</table>

<h3 id="use">Use</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">emergence.hopfield</span> <span class="kn">import</span> <span class="n">Hopfield</span>
<span class="kn">from</span> <span class="nn">emergence.preprocess.preprocess_image</span> <span class="kn">import</span> <span class="o">*</span> 

<span class="c1"># Preprocess data
</span><span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">fashion_mnist</span>
<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">normalize_binarize</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

<span class="c1"># Train network
</span><span class="n">hn</span> <span class="o">=</span> <span class="n">Hopfield</span><span class="p">()</span>
<span class="n">hn</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Get noisy data
</span><span class="n">noise_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">noise_image</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

<span class="c1"># Run network on noisy data
</span><span class="n">data_hat</span> <span class="o">=</span> <span class="p">[</span><span class="n">hn</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">noise_data</span><span class="p">]</span>
</code></pre></div></div>

<p>See /tests for more examples.</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="machine learning" /><category term="neural nets" /><category term="neuroscience" /><category term="computational neuroscience" /><category term="projects" /><summary type="html"><![CDATA[A library of Hopfield networks and undirected neural nets.]]></summary></entry><entry><title type="html">Ideas for the deep learning framework in neuroscience</title><link href="http://localhost:4000/blog/deep-learning-neuro/" rel="alternate" type="text/html" title="Ideas for the deep learning framework in neuroscience" /><published>2020-04-29T00:00:00-07:00</published><updated>2020-04-29T00:00:00-07:00</updated><id>http://localhost:4000/blog/deep-learning-neuro</id><content type="html" xml:base="http://localhost:4000/blog/deep-learning-neuro/"><![CDATA[<p>Broadly speaking, computational neuroscience can be divided into two camps: models that analyze neural data, and models whose structure mathematically imitates that of the brain. The latter fascinates me, acting as a proof-of-concept for various facets of intelligence. And so it is unsuprising that my encounter with neural nets in college sent me away from molecular neuroscience and deep into foundational AI.</p>

<h1 id="the-goal-driven-technique">The Goal-Driven Technique</h1>

<p>One technique that falls into this latter camp is the ‚Äúgoal-driven‚Äù, or ‚Äúnormative‚Äù deep learning method. Instead of fitting your deep learning model directly to neural data, you train the model on a task that the neural substrate performs. Then, you compare the internal representations between the artificial model with the neural data. If the internal representations correlate throughout the model, then your model may be biologically plausible.</p>

<p>For example, if I wanted to understand the visual stream, I could train a deep CNN to recognize images. Then, I could compare the representations throughout the layers of that artificial neural net to neuron responses in the hierarchy of a macaque visual stream. Because the same problem only has so many solutions, certain artificial neural nets will find a solution that is mathematically analogous to that of the biological visual stream.</p>

<p>This approach has a major advantage over past methods in that the researcher does not have to manually set the parameters of the model. Rather, the parameters emerge organically after optimizing the model to perform a given task. And perhaps most importantly, not only do the outputs of the model correlate with its biological counterpart; the internal representations of the model also match the biology, without any explicit training on neural data. The model is not a black-box: it is interpretable, in that layers of the CNN <a href="https://www.pnas.org/content/111/23/8619">map onto layers of the biological visual stream in terms of predictivity</a>.</p>

<p>We can examine these task-optimized artificial neural nets further. Perhaps they are similar to the brain in ways that are hard to study in a wet lab, and so they can act as inexpensive models for ‚Äúvirtual‚Äù lesion and electrophysiology studies.</p>

<p>Of course, we must also exercise caution‚Äì neural nets and the brain are complex systems, so vastly different internal configurations can lead to the same final outcome. However, with the correct interpretation, this method will lead to rapid advances in our understanding of the brain.</p>

<p>Below I‚Äôve compiled a list of potential research directions in which to take the goal-driven method.</p>

<h1 id="potential-research-directions">Potential Research Directions</h1>

<h2 id="sparsity">Sparsity</h2>

<p>It is well-known that representations in the brain are highly <a href="https://pubmed.ncbi.nlm.nih.gov/22579264/">sparse</a>. The <a href="https://arxiv.org/pdf/1803.03635.pdf">Lottery Ticket Hypothesis</a>, written more about <a href="https://soniajoseph.github.io/pruning/">here</a>, shows that neural nets can retain most of their accuracy depite losing 99% of their weights. Thus fascinating questions arise whether neural pruning mechanisms like <a href="https://pubmed.ncbi.nlm.nih.gov/21778362/">microglia</a> can be modeled by sparsifying artificial neural nets.</p>

<h2 id="movies-environments-and-naturalistic-vision">Movies, Environments, and Naturalistic Vision</h2>

<p>Image datasets are limited in that they do not approximate naturalistic vision, which operates in a 3D environment through time. To give rise to maximally biologically-plausible architectures, it is likely that the model will have to be optimized to recognize objects in a 3D environment that it can interact with. Recent research shows that <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhuang_Unsupervised_Learning_From_Video_With_Deep_Neural_Embeddings_CVPR_2020_paper.pdf">unsupervised learning on video is a promising approach</a>.</p>

<p>More broadly, I suspect that unsupervised methods like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> will become default when using the goal-driven method. Unsupervised methods do not need langauage, as organisms do not necessarily need language to learn categories.</p>

<h2 id="intermediate-gabor-patches">Intermediate ‚ÄúGabor Patches‚Äù</h2>

<figure>
  <img src="/assets/images/posts/deepdream.png" />
  <figcaption>[Source](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html).</figcaption>
</figure>

<p>Applying gradient descent (<a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">‚ÄúDeepDream‚Äù</a>) on the original image is one method to visualize the stimulus that maximally activates a given neuron of an artificial neural net. Just as Gabor patches drive neural activity in the early layers of the biological visual stream, can visualizations like the one above drive neural activity in intermediate layers of the model‚Äôs biological counterpart?</p>

<h2 id="ventrolateral-pfc-in-object-recognition">Ventrolateral PFC in Object Recognition</h2>

<p>The role of the PFC in object recognition is poorly understood, but the area is suspected to encode <a href="https://www.researchgate.net/publication/12122406_Freedman_DJ_Riesenhuber_M_Poggio_T_Miller_EK_Categorical_representation_of_visual_stimuli_in_the_primate_prefrontal_cortex_Science_291_312-316">behaviorally relevant information about the object</a>. Some evidence suggests that the ventrolateral prefrontal cortex (vlPFC) is involved in top-down processes that <a href="https://pubmed.ncbi.nlm.nih.gov/17071109/">aid object recognition</a> and contain <a href="https://psycnet.apa.org/record/1999-03885-004">small populations of domain-specific neurons sensitive to eyes, faces, and words</a>.</p>

<p>One idea is measuring neural response in the vlPFC to an artificial neural net trained on images. We can then manipulate the stimuli to measure activations in the vlPFC. If our goal-driven model is predictive of responses, we can check whether the intermediate layers are also predictive of IT, V4, V2, and V1, in line with the <a href="https://www.pnas.org/content/111/23/8619">original findings for the IT</a>.</p>

<p><em>Read more about using goal-driven neural nets on the prefrontal cortex <a href="https://soniajoseph.github.io/computational%20neuroscience/deep-learning-models-prefrontal-cortex/">here</a></em>.</p>

<h2 id="mapping-out-architecture-space-capsulenets">Mapping Out Architecture Space: CapsuleNets</h2>

<p>Which neural net architectures have the highest predictivity of neural response? Is the architecture biologically plausible?</p>

<p>One interesting candidate is CapsuleNets. <a href="https://www.pnas.org/content/111/23/8619">Previous studies</a> focus on comparing the internal representation of CNNs to that of the visual stream. However, in failing to encode the spatial relationships between features, CNNs lose biological plausibility. Hinton et al created <a href="https://arxiv.org/abs/1710.09829">CapsuleNets</a> as one candidate for a more biologically plausible model.</p>

<p>It‚Äôs possible that the activity vectors and dynamic routing of CapsuleNets provide a more biologically plausible mechanism than the normalization, pooling, thresholding, and convolutions of CNNs. One promising avenue is using the goal-driven method to compare the the similarity between task-optimized CapsuleNets and CNNs to biological neural responses. If CapsuleNets are more predictive of neural response, that may speak favorably for their biological plausibility.</p>

<h2 id="performance-and-predictiveness-the-upside-down-u-shape">Performance and Predictiveness: the Upside-Down U-shape</h2>

<p>Artificial neural nets do not necessarily face the same biological constraints as brains. One example may be an architecture like ResNets, which improves image classification performance but may not be biologically plausible with its long skip-connections.</p>

<p>Thus it is not surprising that the top-performing neural nets on image classification <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">surpass human-level performance</a>. One hypothesis is that the relationship between performance on image classification and predictivity of neural activity follows an upside-down U-shape. Neural nets that are better at image classification will correlate more strongly with IT response‚Äì until an inflection point, when the neural net will surpass biological limitations. From thereon I expect that the correlation to IT response will decrease.</p>

<h2 id="integration-with-energy-based-models">Integration with Energy-Based Models</h2>

<p>Long used as <a href="https://bi.snu.ac.kr/Courses/g-ai09-2/hopfield82.pdf">models for associative memory</a>, energy-based models like Boltzmann machines and Hopfield networks are <a href="https://arxiv.org/abs/2008.02217">slowly making a comeback in modern deep learning</a>. It would be interesting to examine hybrids between deep learning models and energy-based models as biologically plausible candidates.</p>

<h1 id="more-resources">More Resources</h1>

<p>Some good overviews of the goal-driven paradigm are <a href="https://www.nature.com/articles/nn.4244">this original 2014 paper</a>, and <a href="https://oxfordre.com/neuroscience/view/10.1093/acrefore/9780190264086.001.0001/acrefore-9780190264086-e-46">this 2019 overview</a>.</p>

<p>The technique is flourishing in computational neuroscience labs across the world, so I will be missing many labs if I merely listed the ones I knew.</p>

<p>If you‚Äôre looking for a more detailed, hands-on introduction to goal-driven neural nets, Carsen Stringer‚Äôs <a href="https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W3D4_DeepLearning1/W3D4_Tutorial3.ipynb">Jupyter notebook</a> under Neuromatch Academy is an unrivaled resource. Other notebooks in the series also include detailed tutorials on the basics of machine learning and PyTorch.</p>

<p>Please message me if you would like to chat or collaborate in the future on any of these ideas (or none!).</p>

<p><em>Note: Citations updated on July 2020.</em></p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="blog" /><category term="machine learning" /><category term="deep learning" /><category term="computational neuroscience" /><category term="neuroscience" /><summary type="html"><![CDATA[On accounting for billions+ of parameters and nonlinearities]]></summary></entry><entry><title type="html">Semantic Similarity Search for Phrases</title><link href="http://localhost:4000/machine%20learning/semantic-similarity-search-phrases/" rel="alternate" type="text/html" title="Semantic Similarity Search for Phrases" /><published>2020-01-26T00:00:00-08:00</published><updated>2020-01-26T00:00:00-08:00</updated><id>http://localhost:4000/machine%20learning/semantic-similarity-search-phrases</id><content type="html" xml:base="http://localhost:4000/machine%20learning/semantic-similarity-search-phrases/"><![CDATA[<p>Word vector averaging is a way to find semantically similar sentences. The idea is simple: find the word embedding for each word using an algorithm like word2vec or GloVe, average the embeddings together to get a sentence vector, and match sentences with the most similar sentence vectors based off Euclidean distance or cosine similarity.</p>

<p>I recently became interested in extending the idea to the phrase level. If we have two sentences, can we co-locate all the semantically similar phrases from each sentence?</p>

<p>For example, if I have the following two sentences:</p>

<blockquote>
  <p>Sentence A:  Yucaipa owned Dominick‚Äôs before selling the chain to Safeway in 1998 for $2.5 billion.</p>

  <p>Sentence B:  Yucaipa bought Dominick‚Äôs in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.</p>
</blockquote>

<p>I want my algorithm to return similar phrases:</p>

<blockquote>
  <p>‚ÄúYucaipa owned Dominick‚Äôs‚Äù / ‚ÄúYucapia bought Dominick‚Äôs‚Äù</p>

  <p>‚Äúselling the chain to Safeway‚Äù / ‚Äúsold it to Safeway‚Äù</p>

  <p>‚Äúin 1998 for $2.5 billion‚Äù / ‚Äúfor $1.8 billion in 1998‚Äù</p>
</blockquote>

<p>To accomplish this, I wrote an algorithm with the following steps:</p>

<p>1) Parse the sentence into phrases using a statistical dependency parser.</p>

<p>2) Create a word vector average for each phrase using word2vec embeddings.</p>

<p>3) Match the most similar phrases based off closest Euclidean distance between word vector averages.</p>

<p>I‚Äôve explained the steps with code in the blog post below, and you can find a Jupyter notebook <a href="https://github.com/soniajoseph/phrase-similarity">here</a>.</p>

<h2 id="load-the-data">Load the data</h2>

<p>Let‚Äôs test our algorithm on the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">Microsoft Research Paraphrase Corpus</a>, which contains pairs of paraphrased sentences extracted from news articles. We‚Äôll create a list of paraphrase lists:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="s">'''
  Function to get spacy model with medium neural net with the constituency parsing extension
  described in "Constituency Parsing with a Self-Attentive Encoder" (2018)

  Args:
      n (int): Number of paraphrase pairs to load.

  Returns:
      new_list: A nested list of n paraphrase pairs.
  '''</span>
  <span class="n">target_url</span> <span class="o">=</span> <span class="s">'https://raw.githubusercontent.com/wasiahmad/paraphrase_identification/master/dataset/msr-paraphrase-corpus/msr_paraphrase_data.txt'</span>
  <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">target_url</span><span class="p">):</span>
      <span class="c1"># skip first sentence
</span>      <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span> <span class="k">continue</span>
      <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">.</span><span class="n">decode</span><span class="p">()</span>
      <span class="n">sentence</span> <span class="o">=</span>  <span class="n">re</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s">'\t+'</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
      <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
      <span class="c1"># increment counter for number of data
</span>      <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">:</span> <span class="k">break</span> 
    <span class="c1"># turn into nested list
</span>  <span class="n">new_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">new_list</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]])</span>

  <span class="k">print</span><span class="p">(</span><span class="s">"Data loaded"</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">new_list</span>
</code></pre></div></div>

<p>We‚Äôll also load a medium <a href="https://spacy.io/models">spaCy model</a> pretrained on English text:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_model</span><span class="p">():</span>
  <span class="s">'''
  Function to get spacy model with medium neural net.

  Args:
      None

  Returns:
      nlp: A loaded model with constituency parsing functionality.
  '''</span>
  <span class="n">nlp</span> <span class="o">=</span> <span class="n">en_core_web_md</span><span class="p">.</span><span class="n">load</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Model loaded"</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">nlp</span> 
</code></pre></div></div>

<h2 id="load-phrase-parser">Load phrase parser</h2>
<p>The first step in solving this problem is parsing the sentences into phrases. We can use the Stanford Statistical Parser, which we can download <a href="https://nlp.stanford.edu/software/lex-parser.shtml">here</a> to create a dependency parser that turns our sentences into phrases based off pre-trained probabilistic dependencies.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_parser</span><span class="p">():</span>
  <span class="s">'''
  Function to load Stanford parser,

  Args:
      None

  Returns:
      parser: return parser object
  '''</span>
  <span class="n">path_exists</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'/content/stanford-parser-full-2018-10-17'</span><span class="p">)</span> <span class="c1">## put your path to the downloaded parser here
</span>  <span class="k">if</span> <span class="n">path_exists</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Load and configure StanfordParser"</span><span class="p">)</span>
    <span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">nlp</span><span class="p">.</span><span class="n">stanford</span><span class="p">.</span><span class="n">edu</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">stanford</span><span class="o">-</span><span class="n">parser</span><span class="o">-</span><span class="n">full</span><span class="o">-</span><span class="mi">2018</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mf">17.</span><span class="nb">zip</span>
    <span class="err">!</span><span class="n">unzip</span> <span class="n">stanford</span><span class="o">-</span><span class="n">parser</span><span class="o">-</span><span class="n">full</span><span class="o">-</span><span class="mi">2018</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mf">17.</span><span class="nb">zip</span>

  <span class="n">stanford_parser_dir</span> <span class="o">=</span> <span class="s">'/content/stanford-parser-full-2018-10-17'</span>
  <span class="n">path_to_models</span> <span class="o">=</span> <span class="n">stanford_parser_dir</span>  <span class="o">+</span> <span class="s">"/stanford-parser-3.9.2-models.jar"</span>
  <span class="n">path_to_jar</span> <span class="o">=</span> <span class="n">stanford_parser_dir</span>  <span class="o">+</span> <span class="s">"/stanford-parser.jar"</span>
  <span class="n">parser</span><span class="o">=</span><span class="n">StanfordParser</span><span class="p">(</span><span class="n">path_to_models</span><span class="p">,</span> <span class="n">path_to_jar</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">parser</span>
</code></pre></div></div>

<p>Using the parser, we can create a tree traversal object so that we can load the sentences into the parser and then traverse the  tree to extract relevant phrases. The traversal object gathers both noun and prepositional phrases.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Traverse</span><span class="p">():</span>
  <span class="s">'''
  Traverse object to create trees to find noun phrases.
  To use, call traverse_tree() with input from the StanfordParser
  Then call the phrase_strings() function with self.phrases to get the noun phrases of
  the input sentence.
  '''</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">phrases</span> <span class="o">=</span> <span class="p">[]</span>
    
  <span class="k">def</span> <span class="nf">traverse_phrase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">phrases</span><span class="p">):</span> 
      <span class="k">for</span> <span class="n">subtree</span> <span class="ow">in</span> <span class="n">tree</span><span class="p">:</span>
          <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">subtree</span><span class="p">)</span> <span class="o">==</span> <span class="n">nltk</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="n">Tree</span><span class="p">:</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">traverse_phrase</span><span class="p">(</span><span class="n">subtree</span><span class="p">,</span> <span class="n">phrases</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">phrases</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">subtree</span><span class="p">)</span>

  <span class="c1"># traverse the tree to gather noun phrases and prepositional phrases
</span>  <span class="k">def</span> <span class="nf">traverse_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">subtree</span> <span class="ow">in</span> <span class="n">tree</span><span class="p">:</span>
          <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">subtree</span><span class="p">)</span> <span class="o">==</span> <span class="n">nltk</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="n">Tree</span><span class="p">:</span>
              <span class="k">if</span> <span class="n">subtree</span><span class="p">.</span><span class="n">label</span><span class="p">()</span> <span class="o">==</span> <span class="s">'NP'</span> <span class="ow">or</span> <span class="n">subtree</span><span class="p">.</span><span class="n">label</span><span class="p">()</span> <span class="o">==</span> <span class="s">'PP'</span><span class="p">:</span>
                  <span class="bp">self</span><span class="p">.</span><span class="n">traverse_phrase</span><span class="p">(</span><span class="n">subtree</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">phrases</span><span class="p">)</span>
                  <span class="bp">self</span><span class="p">.</span><span class="n">phrases</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
              <span class="k">else</span> <span class="p">:</span>
                  <span class="bp">self</span><span class="p">.</span><span class="n">traverse_tree</span><span class="p">(</span><span class="n">subtree</span><span class="p">)</span>

  <span class="c1"># put noun phrases in list
</span>  <span class="k">def</span> <span class="nf">phrase_strings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phrase_list</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">phrase_list</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">a</span> <span class="k">if</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">a</span>
</code></pre></div></div>

<h2 id="lets-write-a-function-to-calculate-a-semantic-measure-for-the-phrase">Let‚Äôs write a function to calculate a semantic measure for the phrase.</h2>

<p>Now we need a metric to calculate the semantic measure of each phrase. I calculate the word vector of each word of the phrase and then average the words together. Notably, I did not normalize the word vectors before averaging because the different lengths give rise to a weighted average, which <a href="https://arxiv.org/pdf/1805.09209.pdf">Arefyev et al suggests is more accurate</a>.</p>

<p>While research regarding weighted vs unweighted word vector averages is scarce, one theory is that infrequent words are correlated with longer word vectors. Infrequent words may be more poorly represented by the embedding, which is based off distributional frequency, and so having a larger value in the total average may make the overall phrase vector more accurate.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">wva</span><span class="p">(</span><span class="n">string</span><span class="p">):</span>
    <span class="s">'''
    Finds document vector through an average of each word's vector.

    Args: 
      string (str): Input sentence

    Returns:
      array: Word vector average
    '''</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
    <span class="n">wvs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">doc</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">vector</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">))])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wvs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</code></pre></div></div>

<p>Now we will calculate the phrases with the greatest semantic similarity. The most common measures of semantic similarity are cosine similarity and Euclidean distance. Perhaps unconventionally, I chose the latter in order to capture the information in the word average lengths, which Arefyez et al. suggest is significant in representing word frequency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">match_wv_pair</span><span class="p">(</span><span class="n">phrasesA</span><span class="p">,</span> <span class="n">phrasesB</span><span class="p">):</span>
  <span class="s">'''
  Takes two lists of phrases from one sentence each and finds the smallest Euclidean distance for each pair's word vector (non-exclusive).

  Args:
  phraseA (list of str): List of parsed phrases from or sentence A
  phraseB (list of str): List of parsed phrases from sentence B to compare with sentence A 

  Returns:
  matches (list of str): Returns list of matches between the two phrase (surjectively, i.e. multiple phrases can have the same match).

  '''</span>
  <span class="c1"># get word vectors
</span>  <span class="n">wva_a</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">wva_b</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">phrasesA</span><span class="p">:</span>
    <span class="n">wva_a</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wva</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">phrasesB</span><span class="p">:</span>
    <span class="n">wva_b</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wva</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

  <span class="c1"># swap so that shortest is on the outer for loop
</span>  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">wva_a</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">wva_b</span><span class="p">):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">wva_a</span>
    <span class="n">wva_a</span> <span class="o">=</span> <span class="n">wva_b</span>
    <span class="n">wva_b</span> <span class="o">=</span> <span class="n">temp</span>

    <span class="n">temp</span> <span class="o">=</span> <span class="n">phrasesA</span>
    <span class="n">phrasesA</span> <span class="o">=</span> <span class="n">phrasesB</span>
    <span class="n">phrasesB</span> <span class="o">=</span> <span class="n">temp</span>

  <span class="n">matches</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">wva_a</span><span class="p">)):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">wva_b</span><span class="p">)):</span>
      <span class="n">distances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">wva_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">wva_b</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
      <span class="c1"># indices_total.append(np.argsort(distances)[0])
</span>    <span class="n">matches</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"Sentence A: "</span> <span class="o">+</span> <span class="n">phrasesA</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s"> Sentence B:"</span> <span class="o">+</span> <span class="n">phrasesB</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s"> Euclidean Distance:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">matches</span>
</code></pre></div></div>

<h2 id="run-the-final-function">Run the final function</h2>

<p>Finally, I wrote a function that runs the functions above: take in our data and parser, parse the sentences into phrases, turn the phrases into word vector averages, then match word vector averages based on Euclidean distance. The function prints out semantically similar phrases for each paraphrase-pair of the original dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_similar_phrases</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">parser</span><span class="p">):</span>
  <span class="s">'''
  Uses Traverse object to create phrase trees of each sentence, then recurses through tree to collect noun phrases.

  Args: 
  string (list of strings): List of string lists in the format [[a,b],[c,d]] to find similarity between each pair.

  Returns:
  Nothing (prints out original sentences, a phrases, b phrases, matching phrases, and their Euclidean distance)
  '''</span>
  <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Original sentences:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Sentence A: "</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Sentence B: "</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="n">ta</span> <span class="o">=</span> <span class="n">Traverse</span><span class="p">()</span>
    <span class="n">phrasesA</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">raw_parse</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">ta</span><span class="p">.</span><span class="n">traverse_tree</span><span class="p">(</span><span class="n">phrasesA</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ta</span><span class="p">.</span><span class="n">phrases</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ta</span><span class="p">.</span><span class="n">phrase_strings</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="n">tb</span> <span class="o">=</span> <span class="n">Traverse</span><span class="p">()</span>
    <span class="n">phrasesB</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">raw_parse</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">tb</span><span class="p">.</span><span class="n">traverse_tree</span><span class="p">(</span><span class="n">phrasesB</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tb</span><span class="p">.</span><span class="n">phrases</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tb</span><span class="p">.</span><span class="n">phrase_strings</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"A phrases:"</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"B phrases:"</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="n">matches</span> <span class="o">=</span> <span class="n">match_wv_pair</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Similar phrases:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<p>The function returns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">Original</span> <span class="n">sentences</span><span class="p">:</span>
<span class="n">Sentence</span> <span class="n">A</span><span class="p">:</span>  <span class="n">Yucaipa</span> <span class="n">owned</span> <span class="n">Dominick</span><span class="s">'s before selling the chain to Safeway in 1998 for $2.5 billion.
Sentence B:  Yucaipa bought Dominick'</span><span class="n">s</span> <span class="ow">in</span> <span class="mi">1995</span> <span class="k">for</span> <span class="err">$</span><span class="mi">693</span> <span class="n">million</span> <span class="ow">and</span> <span class="n">sold</span> <span class="n">it</span> <span class="n">to</span> <span class="n">Safeway</span> <span class="k">for</span> <span class="err">$</span><span class="mf">1.8</span> <span class="n">billion</span> <span class="ow">in</span> <span class="mf">1998.</span>

<span class="n">A</span> <span class="n">phrases</span><span class="p">:</span> <span class="p">[</span><span class="s">'Yucaipa'</span><span class="p">,</span> <span class="s">'Dominick'</span><span class="p">,</span> <span class="s">'before selling the chain to Safeway in 1998 for $ 2.5 billion'</span><span class="p">]</span>
<span class="n">B</span> <span class="n">phrases</span><span class="p">:</span> <span class="p">[</span><span class="s">'Yucaipa'</span><span class="p">,</span> <span class="s">"Dominick 's in 1995"</span><span class="p">,</span> <span class="s">'for $ 693 million'</span><span class="p">,</span> <span class="s">'it'</span><span class="p">,</span> <span class="s">'to Safeway'</span><span class="p">,</span> <span class="s">'for $ 1.8 billion in 1998'</span><span class="p">]</span>

<span class="n">Similar</span> <span class="n">phrases</span><span class="p">:</span>
<span class="n">Sentence</span> <span class="n">A</span><span class="p">:</span> <span class="n">Yucaipa</span>
<span class="n">Sentence</span> <span class="n">B</span><span class="p">:</span><span class="n">Yucaipa</span>
<span class="n">Euclidean</span> <span class="n">Distance</span><span class="p">:</span><span class="mf">0.0</span>

<span class="n">Sentence</span> <span class="n">A</span><span class="p">:</span> <span class="n">Dominick</span>
<span class="n">Sentence</span> <span class="n">B</span><span class="p">:</span> <span class="n">Dominick</span> <span class="s">'s in 1995
Euclidean Distance:5.5339174

Sentence A: before selling the chain to Safeway in 1998 for $ 2.5 billion
Sentence B: for $ 1.8 billion in 1998
Euclidean Distance: 2.0490212


Original sentences:
Sentence A:  They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.
Sentence B:  On June 10, the ship'</span><span class="n">s</span> <span class="n">owners</span> <span class="n">had</span> <span class="n">published</span> <span class="n">an</span> <span class="n">advertisement</span> <span class="n">on</span> <span class="n">the</span> <span class="n">Internet</span><span class="p">,</span> <span class="n">offering</span> <span class="n">the</span> <span class="n">explosives</span> <span class="k">for</span> <span class="n">sale</span><span class="p">.</span>

<span class="n">A</span> <span class="n">phrases</span><span class="p">:</span> <span class="p">[</span><span class="s">'They'</span><span class="p">,</span> <span class="s">'an advertisement on the Internet on June 10'</span><span class="p">,</span> <span class="s">'the cargo for sale'</span><span class="p">,</span> <span class="s">'he'</span><span class="p">]</span>
<span class="n">B</span> <span class="n">phrases</span><span class="p">:</span> <span class="p">[</span><span class="s">'On June 10'</span><span class="p">,</span> <span class="s">"the ship 's owners"</span><span class="p">,</span> <span class="s">'an advertisement on the Internet'</span><span class="p">,</span> <span class="s">'the explosives'</span><span class="p">,</span> <span class="s">'for sale'</span><span class="p">]</span>

<span class="n">Similar</span> <span class="n">phrases</span><span class="p">:</span>
<span class="n">Sentence</span> <span class="n">A</span><span class="p">:</span> <span class="n">They</span>
<span class="n">Sentence</span> <span class="n">B</span><span class="p">:</span> <span class="n">the</span> <span class="n">ship</span> <span class="s">'s owners
Euclidean Distance: 4.217091

Sentence A: an advertisement on the Internet on June 10
Sentence B: an advertisement on the Internet
Euclidean Distance: 1.383867

Sentence A: the cargo for sale
Sentence B: for sale
Euclidean Distance: 2.6837785

Sentence A: he
Sentence B: the ship '</span><span class="n">s</span> <span class="n">owners</span>
<span class="n">Euclidean</span> <span class="n">Distance</span><span class="p">:</span><span class="mf">5.162956</span>
</code></pre></div></div>

<p>To improve the function, we could train a dependency parser on the corpus to extract better phrases. We could also experiment with other ways of traversing the existing tree and trying other similarity measures, including cosine similarity, Mahalanobis distance, and relaxed word mover‚Äôs distance.</p>

<p>I am also interested in experimenting with accuracy in normalized vs. unnormalized word vector averages and the relationship between vector length and word frequency.</p>

<p>I hope this tutorial was helpful to you! The full code for this post is available on Github <a href="https://github.com/soniajoseph/phrase-similarity">here</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="machine learning" /><category term="machine learning" /><category term="natural language processing" /><category term="projects" /><summary type="html"><![CDATA[Matching similar phrases given two corpuses]]></summary></entry><entry><title type="html">Experiments in Pruning</title><link href="http://localhost:4000/pruning/" rel="alternate" type="text/html" title="Experiments in Pruning" /><published>2020-01-05T00:00:00-08:00</published><updated>2020-01-05T00:00:00-08:00</updated><id>http://localhost:4000/pruning</id><content type="html" xml:base="http://localhost:4000/pruning/"><![CDATA[<h2 id="the-unexpected-effects-of-pruning-neural-nets">The Unexpected Effects of Pruning Neural Nets</h2>

<figure class="">
  <img src="/assets/images/posts/dropout.png" alt="Neural net before and after dropout from Srivastava, Nitish, et al. 'Dropout: a simple way to prevent neural networks from overfitting', JMLR 2014" />
  </figure>

<p><em>This project was done as a challenge for <a href="for.ai">for.ai</a>, a multi-disciplinary distributed artificial intelligence research collaboration. I entered the challenge with minimal background in pruning neural nets and consulted the literature only post-experiment.</em></p>

<p><em>The full code on GitHub is <a href="https://github.com/soniajoseph/Pruning">here</a>.</em></p>

<p>Pruning is deleting connections in a neural net in order to improve generalization and reduce computational resources. Two kinds of pruning exist: weight-pruning, in which the largest weights by absolute value are set to zero; and unit-pruning, in which the smallest neurons are set to zero by a vector-wise metric like L2-norm.</p>

<p>Here, I examine the relationship between pruning and accuracy on a vanilla neural net. Before running any experiments, I hypothesize that accuracy for the pruned neural net will slightly rise (due to the regularization), and then have a negative linear correlation with the amount pruned. I also hypothesize that unit-pruning, in deleting entire neurons instead of individual weights, will have a more dramatic negative effect than weight-pruning.</p>

<h2 id="first-lets-load-normalize-and-visualize-the-mnist-dataset">First, let‚Äôs load, normalize, and visualize the MNIST dataset.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> 
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_MNIST</span><span class="p">():</span>
  <span class="s">"""Function to load and normalize MNIST data"""</span> 
  <span class="n">train</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,)),</span>
                                <span class="p">]))</span>
  <span class="n">test</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,)),</span>
                                <span class="p">]))</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"MNIST datset loaded and normalized."</span><span class="p">)</span>
  <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"PyTorch DataLoaders loaded."</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">visualize_MNIST</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
  <span class="s">"""Function to visualize data given a DataLoader object"""</span>
  <span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
  <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="p">.</span><span class="nb">next</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"image shape:"</span><span class="p">,</span> <span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s"> label shape:"</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="c1"># visualize data
</span>  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="p">.</span><span class="n">flatten</span><span class="p">()):</span>
      <span class="n">im_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">plottable_image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">im_idx</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">plottable_image</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load and visualize MNISt
</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_MNIST</span><span class="p">()</span>
<span class="n">visualize_MNIST</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MNIST datset loaded and normalized.
PyTorch DataLoaders loaded.
image shape: torch.Size([100, 1, 28, 28]) 
 label shape: torch.Size([100])
</code></pre></div></div>

<p><img src="Pruning_PyTorch_files/Pruning_PyTorch_5_1.png" alt="png" /></p>

<h2 id="now-lets-build-a-vanilla-neural-net-with-four-hidden-layers-without-pruning">Now let‚Äôs build a vanilla neural net with four hidden layers without pruning.</h2>

<p>We‚Äôll keep things simple and leave out biases, convolutions, and pooling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""A non-sparse neural network with four hidden fully-connected layers"""</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span><span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">hidden1_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">hidden2_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">hidden3_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">hidden4_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden1_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden2_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden3_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden4_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h2 id="lets-train-our-vanilla-neural-net">Let‚Äôs train our vanilla neural net.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="s">"""Function to train a neural net"""</span>

  <span class="n">lossFunction</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
  <span class="n">time0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
  <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span> 

  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Starting epoch"</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
      <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten
</span>      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># forward pass
</span>      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">lossFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span> <span class="c1"># calculate loss
</span>      <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># backpropagate
</span>      <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># update weights
</span>
      <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

      <span class="k">if</span> <span class="n">idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Running loss:"</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">)</span>

  <span class="n">final_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">time0</span><span class="p">)</span><span class="o">/</span><span class="mi">60</span> 
  <span class="k">print</span><span class="p">(</span><span class="s">"Model trained in "</span><span class="p">,</span> <span class="n">final_time</span><span class="p">,</span> <span class="s">"minutes on "</span><span class="p">,</span> <span class="n">total_samples</span><span class="p">,</span> <span class="s">"samples"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Starting epoch 0
Running loss: 2.3038742542266846
Running loss: 77.4635388404131
Running loss: 109.90784302353859
Running loss: 134.65162767469883
Running loss: 157.67301363497972
Running loss: 180.47914689779282
Starting epoch 1
Running loss: 0.234319806098938
Running loss: 16.288803346455097
Running loss: 32.35535905882716
Running loss: 47.42433784343302
Running loss: 62.21571101620793
Running loss: 76.50204934924841
Starting epoch 2
Running loss: 0.08530954271554947
Running loss: 11.152649360708892
Running loss: 22.78821618296206
Running loss: 33.39046012144536
Running loss: 45.17006475571543
Running loss: 55.31901629595086
Model trained in  2.392067523797353 minutes on  180000 samples
</code></pre></div></div>

<h2 id="now-well-test-our-vanilla-neural-net">Now we‚Äôll test our vanilla neural net.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
  <span class="s">"""Test neural net"""</span>

  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span> 

  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
      <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten
</span>      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
      <span class="n">values</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="n">indices</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># print("Accuracy: ", acc, "% for ", total, "training samples")
</span>
  <span class="k">return</span> <span class="n">acc</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The accuracy of our vanilla NN is"</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">"%"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of our vanilla NN is 97.15 %
</code></pre></div></div>

<h2 id="a-97-accuracy-for-our-vanilla-nn-seems-reasonable-now-lets-do-some-weight-and-unit-pruning">A ~97% accuracy for our vanilla NN seems reasonable. Now let‚Äôs do some weight and unit pruning.</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">sparsify_by_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
  <span class="s">"""Function that takes un-sparsified neural net and does weight-pruning
  by k sparsity"""</span>

  <span class="c1"># make copy of original neural net
</span>  <span class="n">sparse_m</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sparse_m</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span> 
      <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># skip last layer of 5-layer neural net
</span>        <span class="k">break</span> 
      <span class="c1"># change tensor to numpy format, then set appropriate number of smallest weights to zero
</span>      <span class="n">layer_copy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="n">layer_copy</span> <span class="o">=</span> <span class="n">layer_copy</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">layer_copy</span><span class="p">).</span><span class="n">argsort</span><span class="p">()</span> <span class="c1"># get indices of smallest weights by absolute value
</span>      <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span><span class="o">*</span><span class="n">k</span><span class="p">)]</span> <span class="c1"># get k fraction of smallest indices 
</span>      <span class="n">layer_copy</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> 

      <span class="c1"># change weights of model
</span>      <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">layer_copy</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">sparse_m</span>  
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">l2</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">i</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">array</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">sparsify_by_unit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
  <span class="s">"""Creates a k-sparsity model with unit-level pruning that sets columns with smallest L2 to zero."""</span>
  
  <span class="c1"># make copy of original neural net
</span>  <span class="n">sparse_m</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sparse_m</span><span class="p">.</span><span class="n">parameters</span><span class="p">()):</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># skip last layer of 5-layer neural net
</span>      <span class="k">break</span>
    <span class="n">layer_copy</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">([</span><span class="n">l2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">layer_copy</span><span class="p">])</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span><span class="o">*</span><span class="n">k</span><span class="p">)]</span>
    <span class="n">layer_copy</span><span class="p">[</span><span class="n">indices</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">layer_copy</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">sparse_m</span> 

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pruning_accuracies</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prune_type</span><span class="p">):</span>
  <span class="s">""" Takes a model and prune type ("weight" or "unit") and returns a DataFrame of pruning accuracies for given sparsities."""</span>

  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">"sparsity"</span><span class="p">:</span> <span class="p">[],</span> <span class="s">"accuracy"</span><span class="p">:</span> <span class="p">[]})</span>
  <span class="n">sparsities</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.97</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">]</span>

  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sparsities</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">prune_type</span> <span class="o">==</span> <span class="s">"weight"</span><span class="p">:</span>
      <span class="n">new_model</span> <span class="o">=</span> <span class="n">sparsify_by_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">prune_type</span> <span class="o">==</span> <span class="s">"unit"</span><span class="p">:</span>
      <span class="n">new_model</span> <span class="o">=</span> <span class="n">sparsify_by_unit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">"Must specify prune-type."</span><span class="p">)</span>
      <span class="k">return</span> 
    <span class="n">acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">new_model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">append</span><span class="p">({</span><span class="s">"sparsity"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="s">"accuracy"</span><span class="p">:</span> <span class="n">acc</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">df</span> 
</code></pre></div></div>

<h2 id="results">Results</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_weight</span> <span class="o">=</span> <span class="n">get_pruning_accuracies</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"weight"</span><span class="p">)</span>
<span class="n">df_unit</span> <span class="o">=</span> <span class="n">get_pruning_accuracies</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"unit"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracies for Weight Pruning"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_weight</span><span class="p">)</span>

<span class="k">print</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracies for Unit Pruning"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_unit</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracies for Weight Pruning
   sparsity  accuracy
0      0.00     97.15
1      0.25     97.12
2      0.50     97.00
3      0.60     96.90
4      0.70     96.77
5      0.80     94.84
6      0.90     82.43
7      0.95     72.03
8      0.97     64.28
9      0.99     31.88

Accuracies for Unit Pruning
   sparsity  accuracy
0      0.00     97.15
1      0.25     97.14
2      0.50     96.98
3      0.60     96.76
4      0.70     94.63
5      0.80     72.66
6      0.90     36.67
7      0.95     19.29
8      0.97     13.07
9      0.99     10.12
</code></pre></div></div>

<h3 id="lets-plot-our-results">Let‚Äôs plot our results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Accuracy vs Sparsity"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_unit</span><span class="p">[</span><span class="s">"sparsity"</span><span class="p">],</span> <span class="n">df_unit</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Unit-pruning"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_weight</span><span class="p">[</span><span class="s">"sparsity"</span><span class="p">],</span> <span class="n">df_weight</span><span class="p">[</span><span class="s">"accuracy"</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Weight-pruning"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Sparsity (as fraction)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"% Accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<figure class="">
  <img src="/assets/images/posts/pruning_unit_vs_weight.png" alt="" />
  </figure>

<h2 id="discussion-pre-literature-review">Discussion (pre-literature review)</h2>

<p>Clearly, my hypothesis that accuracy will rise and then negatively correlate in a roughly linear way with pruning was incorrect. The figure instead shows a dramatic nonlinear relationship between accuracy and pruning. Accuracy remains roughly constant until dropping off at about 75% sparsity for weight-pruning and until 70% sparsity for unit-pruning. My hypothesis that unit-pruning impacts accuracy more dramatically than weight-pruning held up.</p>

<p>These results are fascinating: Less than 25% of the neural net represents important information about its function. The data also suggest that accuracy may slightly increase with a light amount of pruning (~30%), although I would run on more iterations with a larger dataset to be sure. It would make sense that keeping the net‚Äôs smaller weights reduces its generalization.</p>

<h2 id="literature-review">Literature Review</h2>

<p>Let‚Äôs turn to existing papers to get a better grasp on the pruning phenomenon.</p>

<p>In <a href="https://arxiv.org/pdf/1803.03635.pdf">‚ÄúThe Lottery Ticket Hypothesis‚Äù</a>, the authors put forth the idea:</p>

<blockquote>
  <p>‚ÄúA randomly-initialized, dense neural network contains a subnetwork that is initialized such that‚Äîwhen trained in isolation‚Äîit can match the test accuracy of the
original network after training for at most the same number of iterations.‚Äù</p>

</blockquote>

<p>Pruning the network automatically finds the ‚Äúwinning ticket‚Äù subnetwork, whose accuracy is comparable to that of the fully trained net. The idea is similar to the one proposed in <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf">‚ÄúOptimal Brain Damage‚Äù</a> (which is very strangely named, given that dropout is more similar to synaptic pruning in healthy brains than conventional brain damage), in which the authors prune a network based on second-derivative information.</p>

<h2 id="further-directions">Further Directions</h2>

<p>Questions remain. The theory behind the pruning-accuracy relationship remains an ongoing area of research. Are there other ways of finding these ‚Äúwinning ticket‚Äù subnetworks besides pruning (e.g. directly from the objective function)? Why does one have to train a largely overparameterized network first in order for the winning ticket to arise? Can we find winning ticket subnetwork before training the full network (i.e. during training)?</p>

<p>I am also curious if CNNs, RNNs, and ResNets show the same relationship between pruning and accuracy as the vanilla NN examined here. I am interested in the effect of pruning the weights by magnitude of the entire net (opposed to layer by layer), and using magnitude measures other than absolute value and L2-norm. And what about deleting the largest weights and neurons, opposed to the smallest?</p>

<p>Lastly, I am interested in pruning artificial nets to computationally model <a href="https://en.wikipedia.org/wiki/Synaptic_pruning">synaptic pruning</a> with microglia in biological brains. Synaptic pruning may conserve biological resources, improve brain functioning, or both.</p>]]></content><author><name>{&quot;name&quot;=&gt;&quot;&quot;, &quot;avatar&quot;=&gt;&quot;assets/images/avatar.png&quot;, &quot;bio&quot;=&gt;&quot;computer science&lt;br&gt;computational neuroscience&lt;br&gt;&amp; machine learning&quot;, &quot;email&quot;=&gt;&quot;soniamollyjoseph@gmail.com&quot;, &quot;title_separator&quot;=&gt;&quot;-&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;About&quot;, &quot;url&quot;=&gt;&quot;https://soniajoseph.github.io/about/&quot;}, {&quot;label&quot;=&gt;&quot;Medium&quot;, &quot;url&quot;=&gt;&quot;https://medium.com/@soniamollyjoseph&quot;}, {&quot;label&quot;=&gt;&quot;LinkedIn&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/soniamjoseph/&quot;}, {&quot;label&quot;=&gt;&quot;Twitter&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-twitter-square&quot;}, {&quot;label&quot;=&gt;&quot;Facebook&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-facebook-square&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/soniajoseph&quot;}], &quot;location&quot;=&gt;&quot;Princeton, NJ&quot;}</name><email>soniamollyjoseph@gmail.com</email></author><category term="projects" /><category term="machine learning" /><summary type="html"><![CDATA[When pruning 80% of a neural net does not affect accuracy]]></summary></entry></feed>