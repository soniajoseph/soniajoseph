---
title: Papers for Vision Transformers (ViT) and Mechanistic Interpretability
tags: [machine learning, vision transformers]
excerpt: Papers that give context when exploring mechanistic interpretability on vision transformers.
categories: [machine learning, vision transformers]
toc: false
mathjax: "true"
comments: true
collection: posts
comments: true
classes: wide
---


Winter 2023  
Advised by Dr. Blake Richards

Here is an incomplete list of papers I found helpful to read in developing more context for running mechanistic interpretability on vision transformers (ViTs).


## Vision Transformers

1. Ibrahim Alabdulmohsin et al. [“Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design”](https://arxiv.org/pdf/2305.13035.pdf). In: *arXiv preprint arXiv:2305.13035* (2023).
2. Timothée Darcet et al. [“Vision transformers need registers”](https://arxiv.org/pdf/2309.16588.pdf). In: *arXiv preprint arXiv:2309.16588* (2023).
3. Alexey Dosovitskiy et al. [“An image is worth 16x16 words: Transformers for image recognition at scale”](https://openreview.net/pdf?id=YicbFdNTTy). In: *arXiv preprint arXiv:2010.11929* (2020).
4. Salman Khan et al. [“Transformers in vision: A survey”](https://arxiv.org/pdf/2101.01169.pdf). In: *ACM computing surveys (CSUR)* 54.10s (2022), pp. 1–41.
5. Muhammad Muzammal Naseer et al. [“Intriguing properties of vision transformers”](https://openreview.net/pdf?id=o2mbl-Hmfgd). In: *Advances in Neural Information Processing Systems* 34 (2021), pp. 23296–23308.
6. Maithra Raghu et al. [“Do vision transformers see like convolutional neural networks?”](https://arxiv.org/pdf/2108.08810.pdf). In: *Advances in Neural Information Processing Systems* 34 (2021), pp. 12116–12128.
7. Daquan Zhou et al. [“Understanding the robustness in vision transformers”](https://proceedings.mlr.press/v162/zhou22m/zhou22m.pdf). In: *International Conference on Machine Learning. PMLR.* 2022, pp. 27378–27394.

## Feature Visualization and Interpretability

8. Shan Carter et al. [“Exploring neural networks with activation atlases”](https://distill.pub/2019/activation-atlas/). In: *Distill.* (2019).
9. Haozhe Chen et al. [“Interpreting and Controlling Vision Foundation Models via Text Explanations”](https://arxiv.org/pdf/2310.10591.pdf). In: *arXiv preprint arXiv:2310.10591* (2023).
10. Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. [“Interpreting CLIP’s Image Representation via Text-Based Decomposition”](https://arxiv.org/pdf/2310.05916.pdf). In: *arXiv preprint arXiv:2310.05916* (2023).
11. Robert Geirhos et al. [“Don’t trust your eyes: on the (un) reliability of feature visualizations”](https://arxiv.org/pdf/2306.04719.pdf). In: *arXiv preprint arXiv:2306.04719* (2023).

## Mechanistic Interpretability

12. Kumar K Agrawal et al. [“α-ReQ: Assessing Representation Quality in Self-Supervised Learning by measuring eigenspectrum decay”](https://arxiv.org/pdf/2304.14997.pdf). In: *Advances in Neural Information Processing Systems* 35 (2022), pp. 17626–17638.
13. Trenton Bricken et al. [“Towards monosemanticity: Decomposing language models with dictionary learning”](https://transformer-circuits.pub/2023/monosemantic-features/index.html). Transformer Circuits Thread, 2023.
14. Arthur Conmy et al. [“Towards automated circuit discovery for mechanistic interpretability”](https://arxiv.org/pdf/2209.10652.pdf). In: *arXiv preprint arXiv:2304.14997* (2023).
15. Nelson Elhage et al. [“A mathematical framework for transformer circuits”](https://transformer-circuits.pub/2021/framework/index.html). In: Transformer Circuits Thread 1 (2021).
16. Nelson Elhage et al. [“Toy models of superposition”](https://arxiv.org/ftp/arxiv/papers/2209/2209.10652.pdf). In: *arXiv preprint arXiv :2209.10652* (2022).
17. Kevin Wang et al. [“Interpretability in the wild: a circuit for indirect object identification in gpt-2 small”](https://openreview.net/pdf?id=NpsVSN6o4ul). In: *arXiv preprint arXiv:2211.00593* (2022).

## Training Dynamics and Phase Transitions

18. Eric J Michaud et al. [“The quantization model of neural scaling”](https://arxiv.org/pdf/2303.13506.pdf). In: *arXiv preprint arXiv:2303.13506* (2023).
19. Neel Nanda et al. [“Progress measures for grokking via mechanistic interpretability”](https://arxiv.org/pdf/2301.05217.pdf). In: *arXiv preprint arXiv:2301.05217* (2023).
20. Catherine Olsson et al. [“In-context learning and induction heads”](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html). In: *arXiv preprint arXiv:2209.11895* (2022).
